{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MusicGen EMOTIFY Training \n",
    "\n",
    "**Critical Fixes Applied:**\n",
    "1. ‚úÖ Disabled dropout (causes NaN in training mode)\n",
    "2. ‚úÖ Correct conditioning structure (no unpacking)\n",
    "3. ‚úÖ Proper batch tuple unpacking\n",
    "4. ‚úÖ Pure Float32 training\n",
    "5. ‚úÖ No gradient accumulation\n",
    "6. ‚úÖ Conservative learning rate with warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.modules.conditioners import ConditioningAttributes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration - STABLE SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "# Paths\n",
    "AUDIO_DIR = \"D:/EMOTIFY/audio\"\n",
    "EMOTIFY_CSV = \"D:/EMOTIFY/summed_emotions.csv\"\n",
    "\n",
    "# Training - OPTIMIZED\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 20                    # More epochs\n",
    "SEGMENT_DURATION = 10.0\n",
    "LEARNING_RATE = 5e-6               # ‚Üê Higher (was 1e-6)\n",
    "MIN_LR = 1e-7                      # For cosine schedule\n",
    "WARMUP_STEPS = 50                  # ‚Üê Shorter warmup (was 100)\n",
    "MAX_GRAD_NORM = 1.0                # ‚Üê Less aggressive clipping (was 0.5)\n",
    "\n",
    "# Cosine annealing schedule\n",
    "def get_lr(step, total_steps, warmup_steps, base_lr, min_lr):\n",
    "    if step < warmup_steps:\n",
    "        return base_lr * (step + 1) / warmup_steps\n",
    "    else:\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return min_lr + 0.5 * (base_lr - min_lr) * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "\n",
    "# ‚úÖ CRITICAL: No mixed precision, no gradient accumulation\n",
    "USE_AUTOCAST = False\n",
    "GRADIENT_ACCUMULATION = 1\n",
    "\n",
    "# Audio\n",
    "SAMPLE_RATE = 32000\n",
    "CODEBOOKS = 4\n",
    "CARDINALITY = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Data Processing\n",
    "# ============================================================\n",
    "\n",
    "def process_emotify_csv(csv_path):\n",
    "    \"\"\"Process EMOTIFY CSV to create text descriptions\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    emotion_cols = ['amazement', 'solemnity', 'tenderness', 'nostalgia',\n",
    "                    'calmness', 'power', 'joyful_activation', 'tension', 'sadness']\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        track_id = row['track id']\n",
    "        genre = row['genre']\n",
    "        \n",
    "        # Get emotion scores\n",
    "        emotions = {col: row[col] for col in emotion_cols}\n",
    "        sorted_emotions = sorted(emotions.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_emotions = [e[0] for e in sorted_emotions[:2] if e[1] > 0]\n",
    "        \n",
    "        # Create description\n",
    "        if len(top_emotions) > 0:\n",
    "            emotion_str = \", \".join(top_emotions)\n",
    "            description = f\"A {genre} music track characterized by {emotion_str}.\"\n",
    "        else:\n",
    "            description = f\"A {genre} music track.\"\n",
    "        \n",
    "        processed_data.append({\n",
    "            'track_id': track_id,\n",
    "            'text': description,\n",
    "            'genre': genre,\n",
    "            'top_emotions': top_emotions\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "df_processed = process_emotify_csv(EMOTIFY_CSV)\n",
    "print(f\"\\nProcessed {len(df_processed)} tracks\")\n",
    "print(\"\\nSample descriptions:\")\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA AUGMENTATION\n",
    "# ============================================================\n",
    "\n",
    "class EmotifyDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, segment_duration, sample_rate, augment=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.segment_duration = segment_duration\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_samples = int(segment_duration * sample_rate)\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_path = self.audio_dir / f\"{row['track_id']}.mp3\"\n",
    "        \n",
    "        audio, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Convert to mono\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = audio.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # Resample\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            audio = resampler(audio)\n",
    "        \n",
    "        # Random segment (from different positions each time)\n",
    "        if audio.shape[1] > self.segment_samples:\n",
    "            start = torch.randint(0, audio.shape[1] - self.segment_samples, (1,)).item()\n",
    "            audio = audio[:, start:start + self.segment_samples]\n",
    "        else:\n",
    "            padding = self.segment_samples - audio.shape[1]\n",
    "            audio = F.pad(audio, (0, padding))\n",
    "        \n",
    "        # ‚úÖ DATA AUGMENTATION\n",
    "        if self.augment:\n",
    "            # Random gain (volume)\n",
    "            gain = torch.FloatTensor(1).uniform_(0.8, 1.2).item()\n",
    "            audio = audio * gain\n",
    "            \n",
    "            # Random noise (very small)\n",
    "            if torch.rand(1).item() < 0.3:\n",
    "                noise = torch.randn_like(audio) * 0.005\n",
    "                audio = audio + noise\n",
    "        \n",
    "        # Normalize\n",
    "        audio = audio / (audio.abs().max() + 1e-8)\n",
    "        \n",
    "        return (audio, row['text'])\n",
    "\n",
    "dataset = EmotifyDataset(df_processed, AUDIO_DIR, SEGMENT_DURATION, SAMPLE_RATE)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Batches per epoch: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Model Loading with DROPOUT FIX\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading MusicGen model...\")\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "\n",
    "# Convert to Float32 BEFORE moving to GPU\n",
    "print(\"Converting to Float32...\")\n",
    "model.lm = model.lm.float()\n",
    "model.compression_model = model.compression_model.float()\n",
    "\n",
    "# ‚úÖ CRITICAL FIX: Disable ALL dropout layers\n",
    "print(\"Disabling dropout layers...\")\n",
    "dropout_count = 0\n",
    "for module in model.lm.modules():\n",
    "    if isinstance(module, torch.nn.Dropout):\n",
    "        module.p = 0.0\n",
    "        dropout_count += 1\n",
    "print(f\"‚úì Disabled {dropout_count} dropout layers\")\n",
    "\n",
    "# Move to device\n",
    "model.lm = model.lm.to(device)\n",
    "model.compression_model = model.compression_model.to(device)\n",
    "\n",
    "# Set modes\n",
    "model.lm.train()\n",
    "model.compression_model.eval()\n",
    "\n",
    "# Count parameters\n",
    "trainable_params = sum(p.numel() for p in model.lm.parameters() if p.requires_grad)\n",
    "print(f\"\\n‚úì Model loaded successfully\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model dtype: {next(model.lm.parameters()).dtype}\")\n",
    "print(f\"Model device: {next(model.lm.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPROVED TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "# Configuration\n",
    "LEARNING_RATE = 5e-6\n",
    "MIN_LR = 1e-7\n",
    "WARMUP_STEPS = 50\n",
    "MAX_GRAD_NORM = 1.0\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Calculate total steps\n",
    "total_steps = NUM_EPOCHS * len(dataloader)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.lm.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "def get_lr(step):\n",
    "    \"\"\"Cosine annealing with warmup\"\"\"\n",
    "    if step < WARMUP_STEPS:\n",
    "        return LEARNING_RATE * (step + 1) / WARMUP_STEPS\n",
    "    else:\n",
    "        progress = (step - WARMUP_STEPS) / (total_steps - WARMUP_STEPS)\n",
    "        return MIN_LR + 0.5 * (LEARNING_RATE - MIN_LR) * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "# Disable dropout\n",
    "for module in model.lm.modules():\n",
    "    if isinstance(module, torch.nn.Dropout):\n",
    "        module.p = 0.0\n",
    "if hasattr(model.lm, 'cfg_dropout'):\n",
    "    model.lm.cfg_dropout.p = 0.0\n",
    "\n",
    "model.lm.train()\n",
    "model.compression_model.eval()\n",
    "\n",
    "training_history = {'loss': [], 'grad_norm': [], 'lr': []}\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(f\"Total steps: {total_steps}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE} ‚Üí {MIN_LR}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        audio = batch[0].to(device)\n",
    "        descriptions = batch[1]\n",
    "        \n",
    "        # Encode\n",
    "        with torch.no_grad():\n",
    "            codes, _ = model.compression_model.encode(audio)\n",
    "        \n",
    "        # Conditioning\n",
    "        attrs = [ConditioningAttributes(text={'description': desc}) for desc in descriptions]\n",
    "        \n",
    "        # Forward\n",
    "        output = model.lm.compute_predictions(codes, attrs, None)\n",
    "        logits = output.logits\n",
    "        mask = output.mask\n",
    "        \n",
    "        # Loss on valid positions\n",
    "        B, K, T, card = logits.shape\n",
    "        ce_loss = 0.0\n",
    "        \n",
    "        for k in range(K):\n",
    "            logits_k = logits[:, k, :, :].reshape(-1, card)\n",
    "            targets_k = codes[:, k, :].reshape(-1)\n",
    "            mask_k = mask[:, k, :].reshape(-1)\n",
    "            \n",
    "            valid_logits = logits_k[mask_k]\n",
    "            valid_targets = targets_k[mask_k]\n",
    "            \n",
    "            if valid_logits.numel() > 0:\n",
    "                ce_loss += F.cross_entropy(valid_logits, valid_targets)\n",
    "        \n",
    "        loss = ce_loss / K\n",
    "        \n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            continue\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.lm.parameters(), MAX_GRAD_NORM)\n",
    "        \n",
    "        if torch.isnan(grad_norm) or grad_norm > 100.0:\n",
    "            continue\n",
    "        \n",
    "        # Update LR (cosine schedule)\n",
    "        current_lr = get_lr(global_step)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_lr\n",
    "        \n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Track\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        training_history['loss'].append(loss.item())\n",
    "        training_history['grad_norm'].append(grad_norm.item())\n",
    "        training_history['lr'].append(current_lr)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"  Batch {batch_idx}: loss={loss.item():.4f}, grad={grad_norm:.2f}, lr={current_lr:.2e}\")\n",
    "    \n",
    "    # Epoch summary\n",
    "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    print(f\"\\nEpoch {epoch+1}: avg_loss={avg_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.lm.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, 'musicgen_emotify_best.pt')\n",
    "        print(f\"‚úì New best model saved! (loss={avg_loss:.4f})\")\n",
    "    \n",
    "    # Regular checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.lm.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "    }, f'musicgen_emotify_epoch{epoch+1}.pt')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì TRAINING COMPLETE!\")\n",
    "print(f\"Best loss: {best_loss:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Plot Training Metrics\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(training_history['loss'])\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Gradient norm\n",
    "axes[0, 1].plot(training_history['grad_norm'])\n",
    "axes[0, 1].set_title('Gradient Norm')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Grad Norm')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 0].plot(training_history['lr'])\n",
    "axes[1, 0].set_title('Learning Rate')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('LR')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# # Perplexity\n",
    "# axes[1, 1].plot(training_history['perplexity'])\n",
    "# axes[1, 1].set_title('Perplexity')\n",
    "# axes[1, 1].set_xlabel('Step')\n",
    "# axes[1, 1].set_ylabel('Perplexity')\n",
    "# axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Metrics plotted and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RELOAD TRAINED MODEL FROM CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.modules.conditioners import ConditioningAttributes\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Load base model\n",
    "print(\"Loading base MusicGen model...\")\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "\n",
    "# 2. Convert to Float32\n",
    "model.lm = model.lm.float()\n",
    "model.compression_model = model.compression_model.float()\n",
    "\n",
    "# 3. Move to GPU\n",
    "model.lm = model.lm.to(device)\n",
    "model.compression_model = model.compression_model.to(device)\n",
    "\n",
    "# 4. Load your trained weights\n",
    "# Use the last epoch checkpoint (or best one)\n",
    "checkpoint_path = \"musicgen_emotify_best.pt\"  # ‚Üê Change if different\n",
    "print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.lm.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"‚úì Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "print(f\"‚úì Loss was: {checkpoint.get('loss', 'unknown')}\")\n",
    "\n",
    "# 5. Set to eval mode\n",
    "model.lm.eval()\n",
    "model.compression_model.eval()\n",
    "\n",
    "print(\"\\n‚úì Model ready for generation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE 10 SECOND MUSIC (FIXED!)\n",
    "# ============================================================\n",
    "\n",
    "model.lm.eval()\n",
    "\n",
    "# ‚úÖ CRITICAL: Set duration BEFORE generating\n",
    "model.set_generation_params(\n",
    "    duration=10.0,      # 10 seconds (was defaulting to 1 second!)\n",
    "    top_k=250,          # Sampling parameter\n",
    "    top_p=0.0,          # Disable nucleus sampling\n",
    "    temperature=1.0,    # Creativity (1.0 = balanced)\n",
    "    cfg_coef=3.0        # How strongly to follow the prompt\n",
    ")\n",
    "\n",
    "# Test different prompts\n",
    "prompts = [\n",
    "    \"A classical song that evokes great nostalgia and tenderness\",\n",
    "    \"An electronic track with high energy and joyful activation\",\n",
    "    \"A pop song with high amazement and some tension\",\n",
    "    \"A rock song with high power and some sadness\"\n",
    "]\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Generating: {prompt}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        audio = model.generate(\n",
    "            descriptions=[prompt],\n",
    "            progress=True\n",
    "        )\n",
    "    \n",
    "    print(f\"Generated shape: {audio.shape}\")  # Should be [1, 1, 320000]\n",
    "    \n",
    "    # Save\n",
    "    filename = f\"generated_{i+1}.wav\"\n",
    "    torchaudio.save(filename, audio[0].cpu(), 32000)\n",
    "    print(f\"‚úì Saved: {filename}\")\n",
    "    \n",
    "    # Play\n",
    "    display(Audio(audio[0].cpu().numpy(), rate=32000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE 10 SECOND MUSIC (FIXED!)\n",
    "# ============================================================\n",
    "'''\n",
    "    emotion_cols = ['amazement', 'solemnity', 'tenderness', 'nostalgia',\n",
    "                    'calmness', 'power', 'joyful_activation', 'tension', 'sadness']\n",
    "'''\n",
    "\n",
    "model.lm.eval()\n",
    "\n",
    "# ‚úÖ CRITICAL: Set duration BEFORE generating\n",
    "model.set_generation_params(\n",
    "    duration=10.0,      # 10 seconds (was defaulting to 1 second!)\n",
    "    top_k=250,          # Sampling parameter\n",
    "    top_p=0.0,          # Disable nucleus sampling\n",
    "    temperature=1.0,    # Creativity (1.0 = balanced)\n",
    "    cfg_coef=3.0        # How strongly to follow the prompt\n",
    ")\n",
    "\n",
    "# Test different prompts\n",
    "prompts = [\n",
    "    \"A classical song that evokes great nostalgia\",\n",
    "    \"A classical song that evokes great tenderness\",\n",
    "    \"A classical song that evokes great nostalgia and tenderness\",\n",
    "    \n",
    "    \"A classical song that evokes calmness\",\n",
    "    \"A classical song that evokes solemnity\",\n",
    "    \"A classical song that evokes calmness and solemnity\",\n",
    "    \n",
    "    \"A classical track with high joyful activation\",\n",
    "    \"A classical track with high tension\",\n",
    "    \"A classical track with high joyful activation and tension\",\n",
    "    \n",
    "    \"A classical song with high power\",\n",
    "    \"A classical song with high sadness\",\n",
    "    \"A classical song with high power and some sadness\"\n",
    "]\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Generating: {prompt}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        audio = model.generate(\n",
    "            descriptions=[prompt],\n",
    "            progress=True\n",
    "        )\n",
    "    \n",
    "    print(f\"Generated shape: {audio.shape}\")  # Should be [1, 1, 320000]\n",
    "    \n",
    "    # Save\n",
    "    filename = f\"generated_classical_emotional_comparison{i+1}.wav\"\n",
    "    torchaudio.save(filename, audio[0].cpu(), 32000)\n",
    "    print(f\"‚úì Saved: {filename}\")\n",
    "    \n",
    "    # Play\n",
    "    display(Audio(audio[0].cpu().numpy(), rate=32000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BASE MODEL vs FINE-TUNED MODEL COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from audiocraft.models import MusicGen\n",
    "from IPython.display import Audio, display\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ============================================================\n",
    "# 1. Load BASE model (no fine-tuning)\n",
    "# ============================================================\n",
    "print(\"Loading BASE model...\")\n",
    "base_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "\n",
    "base_model.lm = base_model.lm.float()\n",
    "base_model.compression_model = base_model.compression_model.float()\n",
    "\n",
    "base_model.lm = base_model.lm.to(device)\n",
    "base_model.compression_model = base_model.compression_model.to(device)\n",
    "\n",
    "base_model.lm.eval()\n",
    "base_model.compression_model.eval()\n",
    "\n",
    "base_model.set_generation_params(duration=10.0, top_k=250, cfg_coef=3.0)\n",
    "print(\"‚úì Base model loaded\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Load FINE-TUNED model\n",
    "# ============================================================\n",
    "print(\"\\nLoading FINE-TUNED model...\")\n",
    "finetuned_model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "\n",
    "finetuned_model.lm = finetuned_model.lm.float()\n",
    "finetuned_model.compression_model = finetuned_model.compression_model.float()\n",
    "\n",
    "finetuned_model.lm = finetuned_model.lm.to(device)\n",
    "finetuned_model.compression_model = finetuned_model.compression_model.to(device)\n",
    "\n",
    "checkpoint = torch.load('musicgen_emotify_best.pt', map_location=device)\n",
    "finetuned_model.lm.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "finetuned_model.lm.eval()\n",
    "finetuned_model.compression_model.eval()\n",
    "\n",
    "finetuned_model.set_generation_params(duration=10.0, top_k=250, cfg_coef=3.0)\n",
    "print(f\"‚úì Fine-tuned model loaded (loss={checkpoint.get('loss', 'N/A'):.4f})\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. Test prompts\n",
    "# ============================================================\n",
    "prompts = [\n",
    "    \"A classical song that evokes great nostalgia\",\n",
    "    \"A classical song that evokes great tenderness\",\n",
    "    \"A classical song that evokes great nostalgia and tenderness\",\n",
    "    \n",
    "    \"A classical song that evokes calmness\",\n",
    "    \"A classical song that evokes solemnity\",\n",
    "    \"A classical song that evokes calmness and solemnity\",\n",
    "    \n",
    "    \"A classical track with high joyful activation\",\n",
    "    \"A classical track with high tension\",\n",
    "    \"A classical track with high joyful activation and tension\",\n",
    "    \n",
    "    \"A classical song with high power\",\n",
    "    \"A classical song with high sadness\",\n",
    "    \"A classical song with high power and some sadness\"\n",
    "]\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"comparison_base\", exist_ok=True)\n",
    "os.makedirs(\"comparison_finetuned\", exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# 4. Generate from BOTH models\n",
    "# ============================================================\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{i+1}/{len(prompts)}] {prompt}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    # Clean filename\n",
    "    safe_name = prompt.replace(\" \", \"_\").replace(\",\", \"\")[:50]\n",
    "    \n",
    "    # --- BASE MODEL ---\n",
    "    print(\"\\nüîµ BASE MODEL generating...\")\n",
    "    with torch.no_grad():\n",
    "        base_audio = base_model.generate(descriptions=[prompt], progress=True)\n",
    "    \n",
    "    base_filename = f\"comparison_base/{i+1:02d}_{safe_name}.wav\"\n",
    "    torchaudio.save(base_filename, base_audio[0].cpu(), 32000)\n",
    "    print(f\"‚úì Saved: {base_filename}\")\n",
    "    \n",
    "    print(\"BASE MODEL output:\")\n",
    "    display(Audio(base_audio[0].cpu().numpy(), rate=32000))\n",
    "    \n",
    "    # --- FINE-TUNED MODEL ---\n",
    "    print(\"\\nüü¢ FINE-TUNED MODEL generating...\")\n",
    "    with torch.no_grad():\n",
    "        finetuned_audio = finetuned_model.generate(descriptions=[prompt], progress=True)\n",
    "    \n",
    "    finetuned_filename = f\"comparison_finetuned/{i+1:02d}_{safe_name}.wav\"\n",
    "    torchaudio.save(finetuned_filename, finetuned_audio[0].cpu(), 32000)\n",
    "    print(f\"‚úì Saved: {finetuned_filename}\")\n",
    "    \n",
    "    print(\"FINE-TUNED MODEL output:\")\n",
    "    display(Audio(finetuned_audio[0].cpu().numpy(), rate=32000))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
